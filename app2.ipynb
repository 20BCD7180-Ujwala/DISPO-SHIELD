{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kk_qtJgEC4BY"
      },
      "outputs": [],
      "source": [
        "from tracemalloc import stop\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "sw=nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "rad=st.sidebar.radio(\"Navigation\",[\"Home\",\"Spam/Ham Detection\",\"Sentiment Analysis\",\"Stress Detection\",\"Hate and Offensive Content Detection\",\"Sarcasm Detection\"])\n",
        "\n",
        "#Home Page\n",
        "if rad==\"Home\":\n",
        "    st.title(\"DISPO SHIELD - A Multi-faceted IntelliText Analyser\")\n",
        "    st.image(\"Home Page.png\")\n",
        "    st.text(\" \")\n",
        "    st.text(\"The Following Text Analysis Options Are Available->\")\n",
        "    st.text(\" \")\n",
        "    st.text(\"1. Spam/Ham Detection\")\n",
        "    st.text(\"2. Sentiment Analysis\")\n",
        "    st.text(\"3. Stress Detection\")\n",
        "    st.text(\"4. Hate and Offensive Content Detection\")\n",
        "    st.text(\"5. Sarcasm Detection\")\n",
        "\n",
        "#function to clean and transform the user input which is in raw format\n",
        "def transform_text(text):\n",
        "    text=text.lower()\n",
        "    text=nltk.word_tokenize(text)\n",
        "    y=[]\n",
        "    for i in text:\n",
        "        if i.isalnum():\n",
        "            y.append(i)\n",
        "    text=y[:]\n",
        "    y.clear()\n",
        "    for i in text:\n",
        "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
        "            y.append(i)\n",
        "    text=y[:]\n",
        "    y.clear()\n",
        "    ps=PorterStemmer()\n",
        "    for i in text:\n",
        "        y.append(ps.stem(i))\n",
        "    return \" \".join(y)\n",
        "\n",
        "#Spam Detection Prediction\n",
        "tfidf1=TfidfVectorizer(stop_words=sw,max_features=20)\n",
        "def transform1(txt1):\n",
        "    txt2=tfidf1.fit_transform(txt1)\n",
        "    return txt2.toarray()\n",
        "\n",
        "df1=pd.read_csv(\"Spam Detection.csv\")\n",
        "df1.columns=[\"Label\",\"Text\"]\n",
        "x=transform1(df1[\"Text\"])\n",
        "y=df1[\"Label\"]\n",
        "x_train1,x_test1,y_train1,y_test1=train_test_split(x,y,test_size=0.1,random_state=0)\n",
        "model1=LogisticRegression()\n",
        "model1.fit(x_train1,y_train1)\n",
        "\n",
        "#Spam Detection Analysis Page\n",
        "if rad==\"Spam/Ham Detection\":\n",
        "    st.header(\"Detect Whether A Text Is Spam Or Ham??\")\n",
        "    sent1=st.text_area(\"Enter The Text\")\n",
        "    transformed_sent1=transform_text(sent1)\n",
        "    vector_sent1=tfidf1.transform([transformed_sent1])\n",
        "    prediction1=model1.predict(vector_sent1)[0]\n",
        "\n",
        "    if st.button(\"Predict\"):\n",
        "        if prediction1==\"spam\":\n",
        "            st.warning(\"Spam Text!!\")\n",
        "        elif prediction1==\"ham\":\n",
        "            st.success(\"Ham Text!!\")\n",
        "\n",
        "#Sentiment Analysis Prediction\n",
        "tfidf2=TfidfVectorizer(stop_words=sw,max_features=20)\n",
        "def transform2(txt1):\n",
        "    txt2=tfidf2.fit_transform(txt1)\n",
        "    return txt2.toarray()\n",
        "\n",
        "df2=pd.read_csv(\"Sentiment Analysis.csv\")\n",
        "df2.columns=[\"Text\",\"Label\"]\n",
        "x=transform2(df2[\"Text\"])\n",
        "y=df2[\"Label\"]\n",
        "x_train2,x_test2,y_train2,y_test2=train_test_split(x,y,test_size=0.1,random_state=0)\n",
        "model2=LogisticRegression()\n",
        "model2.fit(x_train2,y_train2)\n",
        "\n",
        "#Sentiment Analysis Page\n",
        "if rad==\"Sentiment Analysis\":\n",
        "    st.header(\"Detect The Sentiment Of The Text!!\")\n",
        "    sent2=st.text_area(\"Enter The Text\")\n",
        "    transformed_sent2=transform_text(sent2)\n",
        "    vector_sent2=tfidf2.transform([transformed_sent2])\n",
        "    prediction2=model2.predict(vector_sent2)[0]\n",
        "\n",
        "    if st.button(\"Predict\"):\n",
        "        if prediction2==0:\n",
        "            st.warning(\"Negative Text!!\")\n",
        "        elif prediction2==1:\n",
        "            st.success(\"Positive Text!!\")\n",
        "\n",
        "#Stress Detection Prediction\n",
        "tfidf3=TfidfVectorizer(stop_words=sw,max_features=20)\n",
        "def transform3(txt1):\n",
        "    txt2=tfidf3.fit_transform(txt1)\n",
        "    return txt2.toarray()\n",
        "\n",
        "df3=pd.read_csv(\"Stress Detection.csv\")\n",
        "df3=df3.drop([\"subreddit\",\"post_id\",\"sentence_range\",\"syntax_fk_grade\"],axis=1)\n",
        "df3.columns=[\"Text\",\"Sentiment\",\"Stress Level\"]\n",
        "x=transform3(df3[\"Text\"])\n",
        "y=df3[\"Stress Level\"].to_numpy()\n",
        "x_train3,x_test3,y_train3,y_test3=train_test_split(x,y,test_size=0.1,random_state=0)\n",
        "model3=DecisionTreeRegressor(max_leaf_nodes=2000)\n",
        "model3.fit(x_train3,y_train3)\n",
        "\n",
        "#Stress Detection Page\n",
        "if rad==\"Stress Detection\":\n",
        "    st.header(\"Detect The Amount Of Stress In The Text!!\")\n",
        "    sent3=st.text_area(\"Enter The Text\")\n",
        "    transformed_sent3=transform_text(sent3)\n",
        "    vector_sent3=tfidf3.transform([transformed_sent3])\n",
        "    prediction3=model3.predict(vector_sent3)[0]\n",
        "\n",
        "    if st.button(\"Predict\"):\n",
        "        if prediction3>=0:\n",
        "            st.warning(\"Stressful Text!!\")\n",
        "        elif prediction3<0:\n",
        "            st.success(\"Not A Stressful Text!!\")\n",
        "\n",
        "#Hate & Offensive Content Prediction\n",
        "tfidf4=TfidfVectorizer(stop_words=sw,max_features=20)\n",
        "def transform4(txt1):\n",
        "    txt2=tfidf4.fit_transform(txt1)\n",
        "    return txt2.toarray()\n",
        "\n",
        "df4=pd.read_csv(\"Hate Content Detection.csv\")\n",
        "df4=df4.drop([\"Unnamed: 0\",\"count\",\"neither\"],axis=1)\n",
        "df4.columns=[\"Hate Level\",\"Offensive Level\",\"Class Level\",\"Text\"]\n",
        "x=transform4(df4[\"Text\"])\n",
        "y=df4[\"Class Level\"]\n",
        "x_train4,x_test4,y_train4,y_test4=train_test_split(x,y,test_size=0.1,random_state=0)\n",
        "model4=RandomForestClassifier()\n",
        "model4.fit(x_train4,y_train4)\n",
        "\n",
        "#Hate & Offensive Content Page\n",
        "if rad==\"Hate and Offensive Content Detection\":\n",
        "    st.header(\"Detect The Level Of Hate & Offensive Content In The Text!!\")\n",
        "    sent4=st.text_area(\"Enter The Text\")\n",
        "    transformed_sent4=transform_text(sent4)\n",
        "    vector_sent4=tfidf4.transform([transformed_sent4])\n",
        "    prediction4=model4.predict(vector_sent4)[0]\n",
        "\n",
        "    if st.button(\"Predict\"):\n",
        "        if prediction4==0:\n",
        "            st.exception(\"Highly Offensive Text!!\")\n",
        "        elif prediction4==1:\n",
        "            st.warning(\"Offensive Text!!\")\n",
        "        elif prediction4==2:\n",
        "            st.success(\"Non Offensive Text!!\")\n",
        "\n",
        "#Sarcasm Detection Prediction\n",
        "tfidf5=TfidfVectorizer(stop_words=sw,max_features=20)\n",
        "def transform5(txt1):\n",
        "    txt2=tfidf5.fit_transform(txt1)\n",
        "    return txt2.toarray()\n",
        "\n",
        "df5=pd.read_csv(\"Sarcasm Detection.csv\")\n",
        "df5.columns=[\"Text\",\"Label\"]\n",
        "x=transform5(df5[\"Text\"])\n",
        "y=df5[\"Label\"]\n",
        "x_train5,x_test5,y_train5,y_test5=train_test_split(x,y,test_size=0.1,random_state=0)\n",
        "model5=LogisticRegression()\n",
        "model5.fit(x_train5,y_train5)\n",
        "\n",
        "#Sarcasm Detection Page\n",
        "if rad==\"Sarcasm Detection\":\n",
        "    st.header(\"Detect Whether The Text Is Sarcastic Or Not!!\")\n",
        "    sent5=st.text_area(\"Enter The Text\")\n",
        "    transformed_sent5=transform_text(sent5)\n",
        "    vector_sent5=tfidf5.transform([transformed_sent5])\n",
        "    prediction5=model5.predict(vector_sent5)[0]\n",
        "\n",
        "    if st.button(\"Predict\"):\n",
        "        if prediction5==1:\n",
        "            st.exception(\"Sarcastic Text!!\")\n",
        "        elif prediction5==0:\n",
        "            st.success(\"Non Sarcastic Text!!\")"
      ]
    }
  ]
}